services:
  bge-m3-embedding:
    container_name: bge-m3-tei
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.9
    restart: unless-stopped
    ports:
      - "${EMBEDDING_PORT}:80"
    volumes:
      # Cache directory for downloaded models
      - ./storage/embedding:/data
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HUB_VERBOSITY=debug
    command: >
      --model-id BAAI/bge-m3
      --pooling cls
      --max-concurrent-requests ${EMBEDDING_MAX_CONCURRENT_REQUESTS:-64}
      --max-batch-tokens ${EMBEDDING_MAX_BATCH_TOKENS:-4096}
      --max-client-batch-size ${EMBEDDING_MAX_CLIENT_BATCH_SIZE:-8}
      --max-batch-requests ${EMBEDDING_MAX_BATCH_REQUESTS:-16}
      --tokenization-workers ${EMBEDDING_TOKENIZATION_WORKERS:-2}
      --huggingface-hub-cache /data
    shm_size: 2g
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant_db
    ports:
      - "${QDRANT_HTTP_PORT}:6333"
      - "${QDRANT_GRPS_PORT}:6334"
    volumes:
      - ./storage/qdrant_storage:/qdrant/storage
    restart: unless-stopped
  rag-api:
    build:
      context: .
      dockerfile: server/Dockerfile
    container_name: rag_api
    ports:
      - "${RAG_API_PORT}:8000"
    volumes:
      - ./server:/app/server
    environment:
      - EMBEDDING_SERVICE_URL=http://bge-m3-embedding:80
      - QDRANT_URL=http://qdrant:6333
      - COLLECTION_NAME=${COLLECTION_NAME:-documents}
    depends_on:
      - bge-m3-embedding
      - qdrant
    restart: unless-stopped