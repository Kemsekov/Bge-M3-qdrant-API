#Rag settings
RAG_MODEL = 'model'

#for ollama http://localhost:11434/api/chat 
#for openai https://api.z.ai/api/coding/paas/v4/chat/completions
RAG_URL = 'http://ollama:11434/api/chat'
RAG_API_KEY='ollama'
RAG_MAX_TOKENS=1024
RAG_TEMPERATURE=1.0
MAX_REPHRASES=3

# HuggingFace token (optional, for model downloads)
HF_TOKEN=your_huggingface_token_here

# RAG API configuration
RAG_API_PORT=5121

# Qdrant vector database ports
QDRANT_HTTP_PORT=6333
QDRANT_GRPS_PORT=6334

# BGE-M3 embedding service port
EMBEDDING_PORT=6400

# Qdrant collection name for documents
COLLECTION_NAME=documents

# RAG LLM configuration
RAG_MODEL=your_model_name
RAG_URL=http://localhost:11434/api/chat
RAG_API_KEY=your_api_key
RAG_MAX_TOKENS=1024
RAG_TEMPERATURE=1.0

# BGE-M3 Embedding Service - Low Memory Configuration
# These parameters reduce RAM usage from ~14GB to ~3.7GB (~74% reduction)
# Trade-off: Lower throughput under heavy concurrent load
# For single-user RAG or low-traffic deployments, this is perfectly fine

# Max concurrent requests (default: 512, low-mem: 64)
EMBEDDING_MAX_CONCURRENT_REQUESTS=64

# Max tokens per batch (default: 16384, low-mem: 4096)
EMBEDDING_MAX_BATCH_TOKENS=4096

# Max inputs per client request (default: 32, low-mem: 8)
EMBEDDING_MAX_CLIENT_BATCH_SIZE=8

# Max requests per batch (default: unset, low-mem: 16)
EMBEDDING_MAX_BATCH_REQUESTS=16

# Tokenization worker threads (default: CPU cores, low-mem: 2)
EMBEDDING_TOKENIZATION_WORKERS=2
